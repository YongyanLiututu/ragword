{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca32c8330647359a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from docx import Document\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from docx import Document as DocxDocument  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c41ac5-e985-4251-b48e-2a411d16dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: ['chroma_db', '.ipynb_checkpoints', 'word1.docx']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"/root/word-rag/word\"\n",
    "print(\"Files in directory:\", os.listdir(directory_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff20d64-4d8f-4890-bcb5-fda7c2c826fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in: /root/word-rag/word\n",
      "Files in directory: ['chroma_db', '.ipynb_checkpoints', 'word1.docx']\n",
      "Attempting to read: /root/word-rag/word/word1.docx\n",
      "Query: Summarize the main points of the documents.\n",
      "Results:\n",
      "Retrieved doc: /root/word-rag/word/word1.docx, Distance: 1.456721544265747\n",
      "Retrieved doc: /root/word-rag/word/word1.docx, Distance: 1.4963897466659546\n",
      "Retrieved doc: /root/word-rag/word/word1.docx, Distance: 1.8577896356582642\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from docx import Document as WordDocument\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "try:\n",
    "    emb_model = SentenceTransformer(model_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SentenceTransformer model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Manually load Word documents\n",
    "def load_word_documents(directory):\n",
    "    all_docs = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".docx\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                print(f\"Attempting to read: {full_path}\")\n",
    "                try:\n",
    "                    doc = WordDocument(full_path)\n",
    "                    text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
    "                    if text.strip():\n",
    "                        all_docs.append({\"content\": text, \"metadata\": {\"source\": full_path}})\n",
    "                    else:\n",
    "                        print(f\"Warning: File {full_path} contains no readable text.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {full_path}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "# Load Word files from directory\n",
    "directory_path = os.path.abspath(\"word\")\n",
    "print(f\"Looking for files in: {directory_path}\")\n",
    "if not os.path.isdir(directory_path):\n",
    "    print(f\"Directory not found: {directory_path}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Files in directory:\", os.listdir(directory_path))\n",
    "\n",
    "all_docs = load_word_documents(directory_path)\n",
    "if not all_docs:\n",
    "    print(\"No Word documents found!\")\n",
    "    exit()\n",
    "\n",
    "# Text splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents([\n",
    "    LangchainDocument(page_content=doc[\"content\"], metadata=doc[\"metadata\"])\n",
    "    for doc in all_docs\n",
    "])\n",
    "\n",
    "# Generate embeddings\n",
    "def create_embeddings(documents):\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            embedding = emb_model.encode(doc.page_content)\n",
    "            embeddings.append(embedding)\n",
    "            metadata.append(doc.metadata)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embedding for document: {doc.metadata}, Error: {e}\")\n",
    "    return np.array(embeddings), metadata\n",
    "\n",
    "embeddings, metadatas = create_embeddings(documents)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index\n",
    "faiss.write_index(index, \"faiss_index\")\n",
    "\n",
    "# Query the FAISS index\n",
    "def search_faiss(query, top_k=5, max_distance=10):\n",
    "    query_embedding = emb_model.encode(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(metadatas) and distances[0][i] < max_distance:\n",
    "            results.append({\"metadata\": metadatas[idx], \"distance\": distances[0][i]})\n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"Summarize the main points of the documents.\"\n",
    "results = search_faiss(query, top_k=5, max_distance=10)\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Results:\")\n",
    "for result in results:\n",
    "    print(f\"Retrieved doc: {result['metadata']['source']}, Distance: {result['distance']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1058f80-b2fd-4981-99a2-53e660cb6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model.save('./local_model')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364b8c2-5c75-42c8-a19c-d842a7e9f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in all_docs:\n",
    "    print(f\"Document content: {doc['content'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3226e1e-09e0-4cc0-a9ff-0704f0f15c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 加载本地或预训练模型（我用的是 all-MiniLM-L6-v2）\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# 测试\n",
    "text = \"test query\"\n",
    "embedding = model.encode(text)\n",
    "\n",
    "print(\"Embedding vector:\", embedding)\n",
    "print(\"Vector length:\", len(embedding))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
