{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T22:51:11.426596Z",
     "start_time": "2024-05-29T22:51:10.289380Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.graph_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenTextSplitter\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMGraphTransformer\n\u001b[1;32m     29\u001b[0m  \u001b[38;5;66;03m# 确保模块已安装\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mneo4j\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphDatabase\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.graph_transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field  # 更新为从 pydantic 导入\n",
    "from typing import Tuple, List, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.graph_transformers import LLMGraphTransformer\n",
    " # 确保模块已安装\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d38a149157078b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T22:59:08.194584Z",
     "start_time": "2024-05-29T22:59:07.657655Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-0125\")\n",
    "emb_model = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "graph = Neo4jGraph()\n",
    "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "question = \"What is paged attention and how is it different from the regular attention? How paged attention helped LLMs in training? And what is its relationship with vLLM?\"\n",
    "# idea: https://github.com/tomasonjo/blogs/blob/master/llm/enhancing_rag_with_graph.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59644ec105435fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T22:58:18.720225Z",
     "start_time": "2024-05-29T22:58:18.716439Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Entities(BaseModel):\n",
    "    # extract entities from text to be used for Neo4j graph query\n",
    "    entities: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the __Entity__ recognizable in a Neo4j graph that appear in the text\"\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    # generate a full text query from the graph query results\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str, q_limit: int, o_limit: int) -> str:\n",
    "    # query the neo4j graph for relationships\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.entities:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:$q_limit})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT $o_limit\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity), \"q_limit\": q_limit, \"o_limit\": o_limit},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00453c35c1d5cca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T22:56:48.157217Z",
     "start_time": "2024-05-29T22:56:47.925259Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load the vector store for standard retriever\n",
    "vector_store = Chroma(\n",
    "    embedding_function=emb_model, \n",
    "    persist_directory=\"pdfs/chroma_db\"\n",
    ")\n",
    "chroma_retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3f5c7fc40a4a0b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:04:35.739970Z",
     "start_time": "2024-05-29T23:04:35.734668Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def retriever(question: str, q_limit: int=2, o_limit: int=20):\n",
    "    \"\"\"\n",
    "    from question generate both structured query (neo4j) and unstructured query (chroma)\n",
    "    \"\"\"\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question, q_limit, o_limit)\n",
    "    unstructured_data = [el.page_content for el in chroma_retriever.invoke(question)]\n",
    "    final_data = \\\n",
    "        f\"\"\"\n",
    "        Structured data:\n",
    "        {structured_data}\n",
    "        Unstructured data:\n",
    "        {\"#Document \". join(unstructured_data)}\n",
    "        \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c5b60184c4cdeeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:04:39.263802Z",
     "start_time": "2024-05-29T23:04:39.243125Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# build the chat chain\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb7ddabfd0a7c2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T23:04:43.191443Z",
     "start_time": "2024-05-29T23:04:39.900059Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: What is paged attention and how is it different from the regular attention? How paged attention helped LLMs in training? And what is its relationship with vLLM?\n",
      "paged attention\n",
      "regular attention\n",
      "LLMs\n",
      "vLLM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PagedAttention is a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory. It differs from regular attention by enabling more efficient memory management and handling of decoding algorithms. PagedAttention helped LLMs in training by reducing memory fragmentation and enabling sharing, allowing for near-zero waste in KV cache memory and flexible sharing of KV. Its relationship with vLLM is that vLLM is a high-throughput LLM serving system that utilizes PagedAttention for efficient memory management.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e964cd7e4549dfa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
